{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802cb35d-9dae-4a64-99e8-1ed94f275efe",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Which Integration Runtime (IR) should be used for copying data from an on-premise database to Azure?\n",
    "For copying data from an on-premise database to Azure, you should use the **Self-hosted Integration Runtime (IR)**. This IR allows you to securely connect to on-premises data sources and transfer data to and from Azure.\n",
    "\n",
    "### 2. Describe Azure Data Lake and its role in a data architecture. How does it differ from Azure Blob Storage?\n",
    "**Azure Data Lake** is a scalable and secure data storage and analytics service designed to handle large volumes of structured and unstructured data. It plays a crucial role in data architecture by providing a centralized repository for storing raw data, which can then be processed and analyzed using various tools and services.\n",
    "\n",
    "**Differences from Azure Blob Storage**:\n",
    "- **Purpose**: Azure Data Lake is optimized for big data analytics, while Azure Blob Storage is a general-purpose object storage service.\n",
    "- **Hierarchical Namespace**: Azure Data Lake supports a hierarchical namespace, which allows for organizing data into directories and subdirectories. Azure Blob Storage uses a flat namespace.\n",
    "- **Performance**: Azure Data Lake is designed for high-performance analytics workloads, whereas Azure Blob Storage is optimized for general-purpose storage.\n",
    "\n",
    "### 3. What is Azure Integration Runtime (IR), and how does it support data movement across different networks?\n",
    "**Azure Integration Runtime (IR)** is a compute infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments. It supports data movement by enabling secure and efficient data transfer between on-premises, cloud, and hybrid environments. Azure IR can be used for data movement, data transformation, and activity dispatch.\n",
    "\n",
    "### 4. Explain Slowly Changing Dimension (SCD) Type 1 in a data warehouse. How does it differ from SCD Type 2?\n",
    "**Slowly Changing Dimension (SCD) Type 1**:\n",
    "- **Definition**: SCD Type 1 overwrites the existing data with new data. It does not maintain any history of changes.\n",
    "- **Use Case**: Suitable for scenarios where historical data is not important, and only the latest information is needed.\n",
    "\n",
    "**Difference from SCD Type 2**:\n",
    "- **SCD Type 2**: Maintains historical data by creating new records for changes. It tracks the history of changes by adding new rows with versioning or effective dates.\n",
    "- **Use Case**: Suitable for scenarios where it is important to keep track of historical changes.\n",
    "\n",
    "### 5. What is an index in a database table? Discuss different types of indexes and their impact on query performance.\n",
    "An **index** in a database table is a data structure that improves the speed of data retrieval operations. It allows the database to find and access data more quickly.\n",
    "\n",
    "**Types of Indexes**:\n",
    "- **Clustered Index**: Determines the physical order of data in the table. Each table can have only one clustered index. It improves the performance of queries that retrieve a range of values.\n",
    "- **Non-Clustered Index**: Does not alter the physical order of data. It creates a separate structure to store the index. A table can have multiple non-clustered indexes. It improves the performance of queries that retrieve specific columns.\n",
    "- **Unique Index**: Ensures that all values in the indexed column are unique. It improves the performance of queries that enforce uniqueness constraints.\n",
    "- **Full-Text Index**: Used for full-text search queries. It improves the performance of text-based searches.\n",
    "\n",
    "### 6. Given two datasets, explain how the number of records will vary for each type of join (Inner Join, Left Join, Right Join, Full Outer Join).\n",
    "- **Inner Join**: Returns only the matching records from both datasets. The number of records will be the intersection of the two datasets.\n",
    "- **Left Join**: Returns all records from the left dataset and the matching records from the right dataset. Non-matching records from the right dataset will have NULL values.\n",
    "- **Right Join**: Returns all records from the right dataset and the matching records from the left dataset. Non-matching records from the left dataset will have NULL values.\n",
    "- **Full Outer Join**: Returns all records when there is a match in either dataset. Non-matching records from both datasets will have NULL values.\n",
    "\n",
    "### 7. What are the Control Flow activities in Azure Data Factory? Explain how they differ from Data Flow activities and their typical use cases.\n",
    "**Control Flow Activities**:\n",
    "- **Definition**: Control Flow activities are used to orchestrate the execution of other activities in a pipeline. They control the flow of execution based on conditions, loops, and dependencies.\n",
    "- **Examples**: Execute Pipeline, If Condition, ForEach, Wait, Web Activity.\n",
    "\n",
    "**Data Flow Activities**:\n",
    "- **Definition**: Data Flow activities are used to perform data transformations within a pipeline. They process and transform data from various sources.\n",
    "- **Examples**: Mapping Data Flow, Wrangling Data Flow.\n",
    "\n",
    "**Typical Use Cases**:\n",
    "- **Control Flow Activities**: Used for orchestrating complex workflows, managing dependencies, and controlling the execution order of activities.\n",
    "- **Data Flow Activities**: Used for transforming and processing data within the pipeline.\n",
    "\n",
    "### 8. Describe the architecture and key components of Azure Data Factory. How do these components interact to orchestrate data workflows?\n",
    "**Architecture and Key Components**:\n",
    "- **Pipelines**: Define the workflow and contain a series of activities.\n",
    "- **Activities**: Perform specific tasks within a pipeline (e.g., data movement, data transformation).\n",
    "- **Datasets**: Represent data structures within data stores (e.g., tables, files).\n",
    "- **Linked Services**: Define the connection information for data sources and destinations.\n",
    "- **Integration Runtimes (IR)**: Provide the compute environment for data movement and transformation.\n",
    "\n",
    "**Interaction**:\n",
    "- Pipelines orchestrate the execution of activities.\n",
    "- Activities use datasets to read from and write to data stores.\n",
    "- Linked Services provide the connection information for accessing data stores.\n",
    "- Integration Runtimes execute the activities and handle data movement and transformation.\n",
    "\n",
    "### 9. What are the different types of Integration Runtimes (IR) in Azure Data Factory? Discuss their use cases and limitations.\n",
    "**Types of Integration Runtimes (IR)**:\n",
    "- **Azure Integration Runtime**: Used for data movement and transformation within Azure. It supports cloud-based data sources and destinations.\n",
    "  - **Use Case**: Cloud-to-cloud data integration.\n",
    "  - **Limitations**: Limited to Azure and cloud-based data sources.\n",
    "\n",
    "- **Self-hosted Integration Runtime**: Used for data movement and transformation between on-premises and cloud environments. It supports on-premises data sources and destinations.\n",
    "  - **Use Case**: Hybrid data integration (on-premises to cloud).\n",
    "  - **Limitations**: Requires installation and maintenance on on-premises infrastructure.\n",
    "\n",
    "- **Azure-SSIS Integration Runtime**: Used for running SQL Server Integration Services (SSIS) packages in Azure.\n",
    "  - **Use Case**: Migrating existing SSIS packages to Azure.\n",
    "  - **Limitations**: Limited to SSIS package execution.\n",
    "\n",
    "### 10. How can you mask sensitive data in Azure SQL Database? What are the different masking techniques available?\n",
    "**Masking Sensitive Data**:\n",
    "- **Dynamic Data Masking (DDM)**: Masks sensitive data in query results without modifying the actual data in the database.\n",
    "\n",
    "**Masking Techniques**:\n",
    "- **Default Mask**: Replaces the original value with a default mask (e.g., `XXXX` for strings).\n",
    "- **Email Mask**: Masks email addresses by showing only the first letter and domain (e.g., `aXXX@domain.com`).\n",
    "- **Custom String Mask**: Replaces the original value with a custom string (e.g., `XXXX-XXXX`).\n",
    "- **Random Number Mask**: Replaces the original value with a random number within a specified range.\n",
    "\n",
    "### 11. What is Azure Data Factory (ADF), and how does it enable ETL and ELT processes in a cloud environment?\n",
    "**Azure Data Factory (ADF)** is a cloud-based data integration service that allows you to create, schedule, and orchestrate data workflows. It enables ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes by providing a platform to move and transform data from various sources to destinations.\n",
    "\n",
    "### 12. Explain the differences between a Scheduled Trigger and a Tumbling Window Trigger in Azure Data Factory. When would you use each?\n",
    "**Scheduled Trigger**:\n",
    "- **Definition**: Executes a pipeline at a specified time or interval.\n",
    "- **Use Case**: Suitable for scenarios where the pipeline needs to run at regular intervals (e.g., daily, hourly).\n",
    "\n",
    "**Tumbling Window Trigger**:\n",
    "- **Definition**: Executes a pipeline at fixed time intervals, with each interval being a discrete, non-overlapping time window.\n",
    "- **Use Case**: Suitable for scenarios where data needs to be processed in distinct time windows (e.g., processing log files generated every hour).\n",
    "\n",
    "### 13. SQL Questions on Window Functions - Rolling Sum and Lag/Lead Based\n",
    "**Window Functions**:\n",
    "- **Rolling Sum**: Calculates a cumulative sum over a specified window of rows.\n",
    "  ```sql\n",
    "  SELECT \n",
    "    column,\n",
    "    SUM(column) OVER (ORDER BY column ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rolling_sum\n",
    "  FROM table;\n",
    "  ```\n",
    "\n",
    "- **Lag/Lead**: Accesses data from previous or subsequent rows within the same result set.\n",
    "  ```sql\n",
    "  SELECT \n",
    "    column,\n",
    "    LAG(column, 1) OVER (ORDER BY column) AS previous_value,\n",
    "    LEAD(column, 1) OVER (ORDER BY column) AS next_value\n",
    "  FROM table;\n",
    "  ```\n",
    "\n",
    "**Differences from Traditional Aggregate Functions**:\n",
    "- **Window Functions**: Perform calculations across a set of table rows that are somehow related to the current row. Unlike traditional aggregate functions, window functions do not cause rows to become grouped into a single output row. Instead, the rows retain their separate identities.\n",
    "- **Traditional Aggregate Functions**: Perform calculations on a set of values and return a single value. They group rows into a single output row per group.\n",
    "\n",
    "### 14. What are Linked Services in Azure Data Factory, and how do they facilitate connectivity to various data sources?\n",
    "**Linked Services** in Azure Data Factory are connection strings that define the connection information needed for Data Factory to connect to external data sources. They facilitate connectivity by providing the necessary credentials and configuration settings to access data from various sources such as databases, file systems, APIs, and cloud storage.\n",
    "\n",
    "**Example**:\n",
    "```json\n",
    "{\n",
    "  \"name\": \"AzureBlobStorageLinkedService\",\n",
    "  \"properties\": {\n",
    "    \"type\": \"AzureBlobStorage\",\n",
    "    \"typeProperties\": {\n",
    "      \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=your_account_name;AccountKey=your_account_key;EndpointSuffix=core.windows.net\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 15. Discuss Key Concepts in Data Modeling, Including Normalization and Denormalization. How Do Security Concerns Influence Your Choice of Synapse Table Types in a Given Scenario? Provide an Example of a Scenario-Based ADF Pipeline.\n",
    "**Normalization**:\n",
    "- **Definition**: The process of organizing data to minimize redundancy and improve data integrity. It involves dividing a database into two or more tables and defining relationships between them.\n",
    "- **Benefits**: Reduces data redundancy, improves data integrity, and ensures consistency.\n",
    "\n",
    "**Denormalization**:\n",
    "- **Definition**: The process of combining normalized tables to improve read performance. It involves adding redundant data to speed up complex queries.\n",
    "- **Benefits**: Improves query performance, simplifies data retrieval, and reduces the need for complex joins.\n",
    "\n",
    "**Security Concerns and Synapse Table Types**:\n",
    "- **Data Sensitivity**: For highly sensitive data, use dedicated SQL pools with advanced security features like encryption, row-level security, and dynamic data masking.\n",
    "- **Access Control**: Use dedicated SQL pools for strict access control, including object-level, row-level, and column-level security.\n",
    "- **Compliance Requirements**: Use dedicated SQL pools for scenarios requiring strict compliance with regulations like GDPR, HIPAA, etc.\n",
    "\n",
    "**Example Scenario-Based ADF Pipeline**:\n",
    "Let's consider a scenario where we need to build an ETL pipeline to process sensitive customer data and load it into a secure Synapse table.\n",
    "\n",
    "#### Scenario: Processing Sensitive Customer Data\n",
    "1. **Source**: Customer data stored in an on-premises SQL Server.\n",
    "2. **Destination**: Azure Synapse Analytics dedicated SQL pool.\n",
    "3. **Security Requirements**:\n",
    "   - Data encryption at rest and in transit.\n",
    "   - Row-level security to restrict access based on user roles.\n",
    "   - Dynamic data masking to protect sensitive information.\n",
    "\n",
    "#### ADF Pipeline Steps\n",
    "1. **Copy Activity**:\n",
    "   - **Source**: On-premises SQL Server.\n",
    "   - **Destination**: Staging area in Azure Data Lake Storage (ADLS) with encryption enabled.\n",
    "\n",
    "2. **Data Transformation**:\n",
    "   - **Activity**: Data flow activity to clean and transform the data.\n",
    "   - **Transformation**: Apply necessary transformations and data masking.\n",
    "\n",
    "3. **Load Data**:\n",
    "   - **Activity**: Copy activity to load transformed data into the dedicated SQL pool.\n",
    "   - **Destination**: Synapse table with row-level security and dynamic data masking configured.\n",
    "\n",
    "4. **Monitoring and Alerts**:\n",
    "   - **Activity**: Set up monitoring and alerts to track pipeline execution and handle failures.\n",
    "\n",
    "#### Example ADF Pipeline Code\n",
    "```json\n",
    "{\n",
    "  \"name\": \"CustomerDataPipeline\",\n",
    "  \"properties\": {\n",
    "    \"activities\": [\n",
    "      {\n",
    "        \"name\": \"CopyFromOnPremToADLS\",\n",
    "        \"type\": \"Copy\",\n",
    "        \"inputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"OnPremSQLServer\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"ADLSStaging\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"source\": {\n",
    "            \"type\": \"SqlSource\"\n",
    "          },\n",
    "          \"sink\": {\n",
    "            \"type\": \"AzureBlobFSink\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"TransformData\",\n",
    "        \"type\": \"DataFlow\",\n",
    "        \"inputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"ADLSStaging\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"TransformedData\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"dataFlow\": {\n",
    "            \"referenceName\": \"CustomerDataTransformation\",\n",
    "            \"type\": \"DataFlowReference\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"LoadToSynapse\",\n",
    "        \"type\": \"Copy\",\n",
    "        \"inputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"TransformedData\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"SynapseTable\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"source\": {\n",
    "            \"type\": \"BlobSource\"\n",
    "          },\n",
    "          \"sink\": {\n",
    "            \"type\": \"SqlDWSink\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the ADF pipeline securely processes and loads sensitive customer data into a dedicated SQL pool in Azure Synapse Analytics, ensuring compliance with security requirements. Let me know if you need more details or have any other questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c04b3-8101-418b-a774-0be2b2d1b6d7",
   "metadata": {},
   "source": [
    "### 1. What are some optimization techniques in Spark?\n",
    "Some optimization techniques in Spark include:\n",
    "- **Caching and Persistence**: Use `cache()` or `persist()` to store intermediate results in memory.\n",
    "- **Broadcast Joins**: Use `broadcast()` to optimize joins when one of the tables is small.\n",
    "- **Partitioning**: Repartition data to balance the workload across partitions.\n",
    "- **Bucketing**: Use `bucketBy()` to colocate data with the same key in the same bucket.\n",
    "- **Predicate Pushdown**: Apply filters as early as possible to reduce the amount of data read.\n",
    "- **Avoiding Wide Transformations**: Minimize shuffling by using narrow transformations when possible.\n",
    "- **Using DataFrames and Spark SQL**: Leverage Catalyst optimizer for query optimization.\n",
    "- **Skewed Data Handling**: Use techniques like salting to handle skewed data.\n",
    "- **Speculative Execution**: Enable speculative execution to mitigate the impact of straggler tasks.\n",
    "- **Tuning Spark Configuration Parameters**: Adjust parameters like `spark.executor.memory` and `spark.sql.shuffle.partitions`.\n",
    "\n",
    "### 2. What is Auto Optimize in Databricks?\n",
    "**Auto Optimize** in Databricks is a feature that automatically optimizes the layout and performance of Delta Lake tables. It includes:\n",
    "- **Auto Compaction**: Automatically compacts small files into larger ones to improve read performance.\n",
    "- **Optimize Write**: Automatically optimizes the file layout during write operations to reduce the number of small files.\n",
    "\n",
    "### 3. Can you explain the architecture of Databricks?\n",
    "The architecture of Databricks consists of the following key components:\n",
    "- **Workspace**: An interactive environment for data scientists, engineers, and analysts to collaborate.\n",
    "- **Clusters**: Groups of virtual machines that run Databricks workloads.\n",
    "- **Jobs**: Automated workflows that run notebooks, JARs, or Python scripts.\n",
    "- **Delta Lake**: A storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
    "- **Databricks Runtime**: A set of core components that provide optimized Spark, Delta Lake, and other libraries.\n",
    "\n",
    "### 4. How do you configure clusters in Databricks?\n",
    "To configure clusters in Databricks:\n",
    "1. Go to the **Clusters** tab in the Databricks workspace.\n",
    "2. Click on **Create Cluster**.\n",
    "3. Specify the cluster name, Databricks runtime version, and other configurations such as the number of workers, instance types, and auto-scaling options.\n",
    "4. Click **Create Cluster** to launch the cluster.\n",
    "\n",
    "### 5. What are the different types of clusters used in projects?\n",
    "Different types of clusters used in projects include:\n",
    "- **Interactive Clusters**: Used for interactive data exploration and development.\n",
    "- **Job Clusters**: Used for running scheduled jobs and automated workflows.\n",
    "- **High Concurrency Clusters**: Used for serving multiple concurrent users and workloads.\n",
    "- **Single Node Clusters**: Used for lightweight development and testing.\n",
    "\n",
    "### 6. Which Databricks runtime do you use in your project?\n",
    "The choice of Databricks runtime depends on the specific requirements of the project. Commonly used runtimes include:\n",
    "- **Databricks Runtime**: The standard runtime for most workloads.\n",
    "- **Databricks Runtime for Machine Learning**: Optimized for machine learning workloads with pre-installed libraries.\n",
    "- **Databricks Runtime for Genomics**: Optimized for genomics data processing.\n",
    "\n",
    "### 7. Write a PySpark code snippet to find the second highest salary using DataFrames.\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SecondHighestSalary\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 5000), (2, \"Bob\", 7000), (3, \"Charlie\", 6000), (4, \"David\", 7000)]\n",
    "columns = [\"ID\", \"Name\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Window specification\n",
    "window_spec = Window.orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Find the second highest salary\n",
    "df.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "  .filter(col(\"rank\") == 2) \\\n",
    "  .show()\n",
    "```\n",
    "\n",
    "### 8. How do you read a CSV file in Spark?\n",
    "You can read a CSV file in Spark using the `read` method of the `SparkSession` object:\n",
    "```python\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "### 9. Write a Python code snippet to check if a string is a palindrome.\n",
    "```python\n",
    "def is_palindrome(s):\n",
    "    s = s.lower().replace(\" \", \"\")\n",
    "    return s == s[::-1]\n",
    "\n",
    "# Example usage\n",
    "print(is_palindrome(\"A man a plan a canal Panama\"))  # Output: True\n",
    "```\n",
    "\n",
    "### 10. Suppose you have a source that can provide any number of columns as input. How would you ensure that you can handle this situation without failure?\n",
    "To handle a source with a variable number of columns, you can use a schema inference approach or define a flexible schema that can accommodate different column structures. In Spark, you can use the `inferSchema` option to automatically infer the schema from the data:\n",
    "```python\n",
    "df = spark.read.option(\"inferSchema\", \"true\").csv(\"path/to/data.csv\", header=True)\n",
    "```\n",
    "\n",
    "### 11. What is the Parquet file format, and how does it differ from Delta?\n",
    "**Parquet** is a columnar storage file format optimized for big data processing. It provides efficient data compression and encoding schemes, which improve performance and reduce storage costs.\n",
    "\n",
    "**Delta** is a storage layer that builds on Parquet and adds ACID transactions, schema enforcement, and time travel capabilities. Delta Lake ensures data reliability and consistency, making it suitable for complex data pipelines and analytics.\n",
    "\n",
    "### 12. How do you perform time travel in a Delta table?\n",
    "To perform time travel in a Delta table, you can use the `versionAsOf` or `timestampAsOf` options to query the table at a specific version or timestamp:\n",
    "```python\n",
    "# Query by version\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"path/to/delta/table\")\n",
    "\n",
    "# Query by timestamp\n",
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2023-11-10T00:00:00Z\").load(\"path/to/delta/table\")\n",
    "```\n",
    "\n",
    "### 13. How do you implement DevOps in your project, and what kind of Azure services do you use?\n",
    "To implement DevOps in a project, you can use the following Azure services:\n",
    "- **Azure DevOps**: For source control, CI/CD pipelines, and project management.\n",
    "- **Azure Repos**: For version control using Git.\n",
    "- **Azure Pipelines**: For building, testing, and deploying applications.\n",
    "- **Azure Key Vault**: For managing secrets and sensitive information.\n",
    "- **Azure Monitor**: For monitoring and logging application performance.\n",
    "\n",
    "### 14. What are the different activities and trigger types in Azure Data Factory?\n",
    "**Activities**:\n",
    "- **Copy Activity**: Copies data from a source to a destination.\n",
    "- **Data Flow Activity**: Performs data transformations.\n",
    "- **Lookup Activity**: Retrieves data from a data source.\n",
    "- **Execute Pipeline Activity**: Invokes another pipeline.\n",
    "- **Web Activity**: Calls a REST endpoint.\n",
    "\n",
    "**Trigger Types**:\n",
    "- **Schedule Trigger**: Executes a pipeline at a specified time or interval.\n",
    "- **Tumbling Window Trigger**: Executes a pipeline at fixed time intervals with discrete, non-overlapping windows.\n",
    "- **Event-Based Trigger**: Executes a pipeline in response to events such as file creation or deletion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9668392-aa05-4384-8b15-74277ad77656",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What is Data Smoothing?\n",
    "**Data Smoothing** is a technique used to remove noise from a dataset, making patterns more visible. It involves creating an approximating function that captures important patterns in the data while leaving out noise or other fine-scale structures. Common methods include moving averages, exponential smoothing, and Gaussian smoothing.\n",
    "\n",
    "### 2. Explain the difference between `is` and `==` in Python.\n",
    "- **`is`**: Checks for object identity. It returns `True` if two references point to the same object.\n",
    "  ```python\n",
    "  a = [1, 2, 3]\n",
    "  b = a\n",
    "  print(a is b)  # True\n",
    "  ```\n",
    "- **`==`**: Checks for value equality. It returns `True` if the values of two objects are equal.\n",
    "  ```python\n",
    "  a = [1, 2, 3]\n",
    "  b = [1, 2, 3]\n",
    "  print(a == b)  # True\n",
    "  ```\n",
    "\n",
    "### 3. How to delete duplicate elements from a list?\n",
    "You can delete duplicate elements from a list by converting it to a set and then back to a list:\n",
    "```python\n",
    "my_list = [1, 2, 2, 3, 4, 4, 5]\n",
    "my_list = list(set(my_list))\n",
    "print(my_list)  # Output: [1, 2, 3, 4, 5]\n",
    "```\n",
    "\n",
    "### 4. What is Docker?\n",
    "**Docker** is a platform that allows developers to automate the deployment of applications inside lightweight, portable containers. Containers include the application and all its dependencies, ensuring that it runs consistently across different environments.\n",
    "\n",
    "### 5. Explain Spark Context and Streaming Context.\n",
    "- **Spark Context**: The entry point for any Spark application. It allows you to create RDDs, accumulators, and broadcast variables, and it provides access to Spark's cluster.\n",
    "  ```python\n",
    "  from pyspark import SparkContext\n",
    "  sc = SparkContext(\"local\", \"App Name\")\n",
    "  ```\n",
    "\n",
    "- **Streaming Context**: Used for processing real-time data streams. It is built on top of Spark Context and provides methods to create DStreams (Discretized Streams).\n",
    "  ```python\n",
    "  from pyspark.streaming import StreamingContext\n",
    "  ssc = StreamingContext(sc, 1)  # 1-second batch interval\n",
    "  ```\n",
    "\n",
    "### 6. What are RDDs?\n",
    "**RDDs (Resilient Distributed Datasets)** are the fundamental data structure in Spark. They are immutable, distributed collections of objects that can be processed in parallel. RDDs support two types of operations: transformations (e.g., `map`, `filter`) and actions (e.g., `collect`, `count`).\n",
    "\n",
    "### 7. Difference between Map and Reduce.\n",
    "- **Map**: Applies a function to each element of an RDD and returns a new RDD with the results.\n",
    "  ```python\n",
    "  rdd.map(lambda x: x * 2)\n",
    "  ```\n",
    "\n",
    "- **Reduce**: Aggregates the elements of an RDD using a specified function and returns a single result.\n",
    "  ```python\n",
    "  rdd.reduce(lambda x, y: x + y)\n",
    "  ```\n",
    "\n",
    "### 8. Different types of joins in PySpark.\n",
    "- **Inner Join**: Returns only the matching records from both datasets.\n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"inner\")\n",
    "  ```\n",
    "\n",
    "- **Left Join**: Returns all records from the left dataset and the matching records from the right dataset.\n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"left\")\n",
    "  ```\n",
    "\n",
    "- **Right Join**: Returns all records from the right dataset and the matching records from the left dataset.\n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"right\")\n",
    "  ```\n",
    "\n",
    "- **Full Outer Join**: Returns all records when there is a match in either dataset.\n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"outer\")\n",
    "  ```\n",
    "\n",
    "### 9. How are `startswith` and `endswith` methods used in Python?\n",
    "- **`startswith`**: Checks if a string starts with a specified prefix.\n",
    "  ```python\n",
    "  text = \"Hello, world!\"\n",
    "  print(text.startswith(\"Hello\"))  # True\n",
    "  ```\n",
    "\n",
    "- **`endswith`**: Checks if a string ends with a specified suffix.\n",
    "  ```python\n",
    "  text = \"Hello, world!\"\n",
    "  print(text.endswith(\"world!\"))  # True\n",
    "  ```\n",
    "\n",
    "### 10. What is the DAG Scheduler in PySpark?\n",
    "The **DAG Scheduler** in PySpark is responsible for converting a logical execution plan (DAG) into a physical execution plan. It divides the job into stages and tasks, schedules the tasks on the cluster, and handles task failures and retries.\n",
    "\n",
    "### 11. Write a Python program that counts the number of times each character appears in a given string, considering both lowercase and uppercase characters.\n",
    "```python\n",
    "def count_characters(s):\n",
    "    char_count = {}\n",
    "    for char in s:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    return char_count\n",
    "\n",
    "# Example usage\n",
    "input_string = \"Hello, World!\"\n",
    "result = count_characters(input_string)\n",
    "print(result)  # Output: {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1, ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}\n",
    "```\n",
    "\n",
    "Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1204142e-2fdc-41cb-8af1-daf151a371c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+\n",
      "|ActorId|DirectorId|timestamp|\n",
      "+-------+----------+---------+\n",
      "|      1|         1|        0|\n",
      "|      1|         1|        1|\n",
      "|      1|         1|        2|\n",
      "|      1|         2|        3|\n",
      "|      1|         2|        4|\n",
      "|      2|         1|        5|\n",
      "|      2|         1|        6|\n",
      "+-------+----------+---------+\n",
      "\n",
      "+-------+----------+\n",
      "|ActorId|DirectorId|\n",
      "+-------+----------+\n",
      "|      1|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f97c9-02b2-4223-a0e7-a77c28c92a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
