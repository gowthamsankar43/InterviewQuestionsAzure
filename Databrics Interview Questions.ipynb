{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4017d9e-3d08-4635-9d5a-4283ffc56eeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. How do you invoke one notebook from another in Databricks?\n",
    "In Databricks, you can invoke one notebook from another using two primary methods: `%run` and `dbutils.notebook.run()`. Each method has its own use cases and benefits:\n",
    "\n",
    "1. Using `%run`\n",
    "- **Purpose**: Includes another notebook within the current notebook context. This means any variables or functions defined in the called notebook become available in the calling notebook.\n",
    "- **Usage**: Ideal for modularizing code, such as putting supporting functions in a separate notebook.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  %run /path/to/NotebookB\n",
    "  ```\n",
    "\n",
    "2. Using `dbutils.notebook.run()`\n",
    "- **Purpose**: Runs the specified notebook as a separate job, allowing you to pass parameters and handle return values.\n",
    "- **Usage**: Suitable for building complex workflows and pipelines with dependencies.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "  ```\n",
    "\n",
    "Key Differences\n",
    "- **%run**: Executes the notebook inline, making all its variables and functions available in the current notebook.\n",
    "- **dbutils.notebook.run()**: Runs the notebook as a separate job, allowing parameter passing and return values, but does not share the execution context.\n",
    "\n",
    "You can find more details on these methods in the [Databricks documentation](https://docs.databricks.com/en/notebooks/notebook-workflows.html). Let me know if you need more information or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2f01a-f107-4782-9753-aac6191dca4c",
   "metadata": {},
   "source": [
    "### 2. What methods do you use to access a variable from one notebook in another?\n",
    "To access a variable from one notebook in another in Databricks, you can use the following methods:\n",
    "\n",
    "1. Using `%run`\n",
    "- **Purpose**: Includes another notebook within the current notebook context, making all its variables and functions available.\n",
    "- **Usage**: Ideal for modularizing code and sharing variables across notebooks.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  %run /path/to/NotebookB\n",
    "  # Now you can access variables from NotebookB\n",
    "  print(variable_from_notebookB)\n",
    "  ```\n",
    "\n",
    "2. Using `dbutils.notebook.run()`\n",
    "- **Purpose**: Runs the specified notebook as a separate job, allowing you to pass parameters and handle return values.\n",
    "- **Usage**: Suitable for building complex workflows and passing variables between notebooks.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # Run NotebookB and pass parameters\n",
    "  result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "  # Access the return value from NotebookB\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "3. Using Widgets\n",
    "- **Purpose**: Widgets allow you to pass parameters between notebooks interactively.\n",
    "- **Usage**: Useful for parameterizing notebooks and sharing variables.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # In NotebookA\n",
    "  dbutils.widgets.text(\"input\", \"default_value\", \"Input Widget\")\n",
    "  input_value = dbutils.widgets.get(\"input\")\n",
    "\n",
    "  # In NotebookB\n",
    "  dbutils.widgets.text(\"input\", \"default_value\", \"Input Widget\")\n",
    "  input_value = dbutils.widgets.get(\"input\")\n",
    "  ```\n",
    "\n",
    "4. Using External Storage\n",
    "- **Purpose**: Store variables in external storage (e.g., DBFS, S3, Azure Blob Storage) and read them in another notebook.\n",
    "- **Usage**: Suitable for sharing large datasets or variables that need to persist beyond the notebook session.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # In NotebookA\n",
    "  variable_to_share = \"some_value\"\n",
    "  dbutils.fs.put(\"/path/to/shared_variable.txt\", variable_to_share)\n",
    "\n",
    "  # In NotebookB\n",
    "  shared_variable = dbutils.fs.head(\"/path/to/shared_variable.txt\")\n",
    "  print(shared_variable)\n",
    "  ```\n",
    "\n",
    "These methods provide flexibility in accessing and sharing variables between notebooks in Databricks, depending on your specific use case and requirements. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509a6b7-223f-4bba-899c-769bf2cdeec1",
   "metadata": {},
   "source": [
    "### 3. How do you exit a notebook while returning output data in Databricks?\n",
    "In Databricks, you can exit a notebook while returning output data using the `dbutils.notebook.exit()` method. This method allows you to return a string value from the notebook, which can be used by the calling notebook or job.\n",
    "\n",
    "Example\n",
    "Here's an example of how to use `dbutils.notebook.exit()` to return output data from a notebook:\n",
    "\n",
    "Notebook A (Caller)\n",
    "```python\n",
    "# Run Notebook B and capture the returned output\n",
    "result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "print(\"Output from Notebook B:\", result)\n",
    "```\n",
    "\n",
    "Notebook B (Callee)\n",
    "```python\n",
    "# Perform some operations\n",
    "output_data = \"This is the result from Notebook B\"\n",
    "\n",
    "# Exit the notebook and return the output data\n",
    "dbutils.notebook.exit(output_data)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **Notebook A** runs **Notebook B** using `dbutils.notebook.run()` and captures the returned output.\n",
    "- **Notebook B** performs some operations and then uses `dbutils.notebook.exit()` to return the output data.\n",
    "\n",
    "This approach allows you to pass data between notebooks and build complex workflows in Databricks. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adc931-a064-428c-bf9e-6b061e7f0889",
   "metadata": {},
   "source": [
    "### 4. Can you explain the process of creating internal and external tables in Databricks?\n",
    "Sure! In Databricks, you can create both internal (managed) and external tables. Here's how you can do it:\n",
    "\n",
    "Internal (Managed) Tables\n",
    "- **Definition**: Internal tables are managed by Databricks. Databricks handles the storage and management of the data.\n",
    "- **Storage Location**: Data is stored in the Databricks-managed storage location.\n",
    "- **Usage**: Suitable for data that you want Databricks to manage entirely.\n",
    "\n",
    "Example\n",
    "```sql\n",
    "-- Create an internal table\n",
    "CREATE TABLE internal_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT\n",
    ")\n",
    "USING delta\n",
    "LOCATION '/mnt/delta/internal_table';\n",
    "```\n",
    "\n",
    "External Tables\n",
    "- **Definition**: External tables are not managed by Databricks. You specify the storage location, and Databricks only manages the metadata.\n",
    "- **Storage Location**: Data is stored in an external storage location, such as AWS S3, Azure Blob Storage, or DBFS.\n",
    "- **Usage**: Suitable for data that you want to manage outside of Databricks, or data that is shared across multiple systems.\n",
    "\n",
    "Example\n",
    "```sql\n",
    "-- Create an external table\n",
    "CREATE TABLE external_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT\n",
    ")\n",
    "USING delta\n",
    "LOCATION 's3://my-bucket/external_table';\n",
    "```\n",
    "\n",
    "Key Differences\n",
    "1. **Storage Management**:\n",
    "   - **Internal Tables**: Databricks manages the storage.\n",
    "   - **External Tables**: You manage the storage location.\n",
    "\n",
    "2. **Data Lifecycle**:\n",
    "   - **Internal Tables**: Data is deleted when the table is dropped.\n",
    "   - **External Tables**: Data remains in the external storage even if the table is dropped.\n",
    "\n",
    "3. **Use Cases**:\n",
    "   - **Internal Tables**: Use when you want Databricks to handle storage management.\n",
    "   - **External Tables**: Use when you need to manage storage independently or share data across systems.\n",
    "\n",
    "By understanding these differences, you can choose the appropriate table type based on your data management needs. Let me know if you need more details or have any other questions!m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef86589-4b7f-46ba-bb7e-33d011953377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5. What optimization techniques have you implemented in Spark?\n",
    "Optimizing Spark applications is crucial for improving performance and efficiency. Here are some key optimization techniques I've implemented:\n",
    "\n",
    "1. **Caching and Persistence**\n",
    "- **Purpose**: To avoid recomputation of DataFrames or RDDs that are used multiple times.\n",
    "- **Implementation**: Using `cache()` or `persist()` methods to store intermediate results in memory or disk.\n",
    "  ```python\n",
    "  df.cache()\n",
    "  df.count()  # Triggers caching\n",
    "  ```\n",
    "\n",
    "2. **Broadcast Joins**\n",
    "- **Purpose**: To optimize joins when one of the tables is small enough to fit into memory.\n",
    "- **Implementation**: Using `broadcast()` to broadcast the smaller DataFrame to all worker nodes.\n",
    "  ```python\n",
    "  from pyspark.sql.functions import broadcast\n",
    "  result = large_df.join(broadcast(small_df), \"join_column\")\n",
    "  ```\n",
    "\n",
    "3. **Partitioning**\n",
    "- **Purpose**: To distribute data evenly across partitions and reduce shuffling.\n",
    "- **Implementation**: Repartitioning DataFrames based on the join key or other relevant columns.\n",
    "  ```python\n",
    "  df = df.repartition(\"join_column\")\n",
    "  ```\n",
    "\n",
    "4. **Bucketing**\n",
    "- **Purpose**: To optimize joins and aggregations by colocating data with the same key in the same bucket.\n",
    "- **Implementation**: Using `bucketBy()` and `sortBy()` to create bucketed tables.\n",
    "  ```python\n",
    "  df.write.bucketBy(10, \"join_column\").sortBy(\"join_column\").saveAsTable(\"bucketed_table\")\n",
    "  ```\n",
    "\n",
    "5. **Predicate Pushdown**\n",
    "- **Purpose**: To reduce the amount of data read from storage by pushing down filters to the data source.\n",
    "- **Implementation**: Ensuring that filters are applied as early as possible in the query plan.\n",
    "  ```python\n",
    "  df = spark.read.parquet(\"path/to/data\").filter(\"column > value\")\n",
    "  ```\n",
    "\n",
    "6. **Avoiding Wide Transformations**\n",
    "- **Purpose**: To minimize shuffling and reduce the complexity of the execution plan.\n",
    "- **Implementation**: Using narrow transformations like `map()` and `filter()` instead of wide transformations like `groupByKey()` and `reduceByKey()` when possible.\n",
    "\n",
    "7. **Using DataFrames and Spark SQL**\n",
    "- **Purpose**: To leverage Catalyst optimizer for query optimization.\n",
    "- **Implementation**: Using DataFrames and Spark SQL instead of RDDs for better optimization.\n",
    "  ```python\n",
    "  df = spark.sql(\"SELECT * FROM table WHERE column > value\")\n",
    "  ```\n",
    "\n",
    "8. **Skewed Data Handling**\n",
    "- **Purpose**: To address data skew by distributing skewed keys more evenly across partitions.\n",
    "- **Implementation**: Using techniques like salting to handle skewed data.\n",
    "  ```python\n",
    "  from pyspark.sql.functions import col, concat, lit\n",
    "  skewed_df = skewed_df.withColumn(\"salt\", (col(\"join_column\") % 10).cast(\"string\"))\n",
    "  skewed_df = skewed_df.withColumn(\"join_column_salted\", concat(col(\"join_column\"), col(\"salt\")))\n",
    "  ```\n",
    "\n",
    "9. **Speculative Execution**\n",
    "- **Purpose**: To mitigate the impact of straggler tasks by launching speculative copies.\n",
    "- **Implementation**: Enabling speculative execution in the Spark configuration.\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.speculation\", \"true\")\n",
    "  ```\n",
    "\n",
    "10. **Tuning Spark Configuration Parameters**\n",
    "- **Purpose**: To optimize resource utilization and performance.\n",
    "- **Implementation**: Adjusting parameters like `spark.executor.memory`, `spark.executor.cores`, and `spark.sql.shuffle.partitions`.\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.executor.memory\", \"4g\")\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "  ```\n",
    "\n",
    "By implementing these optimization techniques, you can significantly improve the performance and efficiency of your Spark applications. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ec3a8-4c61-4d29-bd35-028ecc4442e4",
   "metadata": {},
   "source": [
    "### 6. How do you manage failure notifications in your workflows?\n",
    "Managing failure notifications in workflows is crucial for ensuring timely responses to issues and maintaining the reliability of your data pipelines. Here are some methods to handle failure notifications effectively:\n",
    "\n",
    "1. **Email Notifications**\n",
    "- **Purpose**: Send email alerts when a workflow fails.\n",
    "- **Implementation**: Use built-in notification features in workflow orchestration tools like Apache Airflow, Azure Data Factory, or Databricks.\n",
    "- **Example**: In Airflow, you can set up email alerts in the DAG definition.\n",
    "  ```python\n",
    "  from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "  email = EmailOperator(\n",
    "      task_id='send_email',\n",
    "      to='your_email@example.com',\n",
    "      subject='Workflow Failure Alert',\n",
    "      html_content='The workflow has failed.',\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "2. **Slack Notifications**\n",
    "- **Purpose**: Send notifications to a Slack channel for real-time alerts.\n",
    "- **Implementation**: Use Slack APIs or integrations provided by workflow tools.\n",
    "- **Example**: In Airflow, you can use the `SlackAPIPostOperator`.\n",
    "  ```python\n",
    "  from airflow.operators.slack_operator import SlackAPIPostOperator\n",
    "\n",
    "  slack_alert = SlackAPIPostOperator(\n",
    "      task_id='slack_alert',\n",
    "      token='your_slack_token',\n",
    "      channel='#alerts',\n",
    "      text='The workflow has failed.',\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "3. **PagerDuty or Opsgenie**\n",
    "- **Purpose**: Use incident management tools like PagerDuty or Opsgenie to handle critical alerts.\n",
    "- **Implementation**: Integrate these tools with your workflow orchestration system to trigger alerts.\n",
    "- **Example**: In Airflow, you can use the `HttpSensor` to trigger PagerDuty alerts.\n",
    "  ```python\n",
    "  from airflow.sensors.http_sensor import HttpSensor\n",
    "\n",
    "  pagerduty_alert = HttpSensor(\n",
    "      task_id='pagerduty_alert',\n",
    "      endpoint='https://events.pagerduty.com/v2/enqueue',\n",
    "      headers={\"Content-Type\": \"application/json\"},\n",
    "      http_conn_id='pagerduty',\n",
    "      request_params={\n",
    "          \"routing_key\": \"your_routing_key\",\n",
    "          \"event_action\": \"trigger\",\n",
    "          \"payload\": {\n",
    "              \"summary\": \"Workflow Failure Alert\",\n",
    "              \"severity\": \"critical\",\n",
    "              \"source\": \"Airflow\",\n",
    "              \"component\": \"workflow\"\n",
    "          }\n",
    "      },\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "4. **Logging and Monitoring**\n",
    "- **Purpose**: Use logging and monitoring tools to track workflow execution and failures.\n",
    "- **Implementation**: Integrate with tools like Datadog, Prometheus, or CloudWatch to monitor workflows and set up alerts.\n",
    "- **Example**: In Databricks, you can use the `dbutils.notebook.exit()` method to log the status of a notebook.\n",
    "  ```python\n",
    "  dbutils.notebook.exit(\"Workflow failed due to an error.\")\n",
    "  ```\n",
    "\n",
    "5. **Retry Mechanisms**\n",
    "- **Purpose**: Automatically retry failed tasks to handle transient issues.\n",
    "- **Implementation**: Configure retry policies in your workflow orchestration tool.\n",
    "- **Example**: In Airflow, you can set the `retries` parameter in the task definition.\n",
    "  ```python\n",
    "  task = PythonOperator(\n",
    "      task_id='my_task',\n",
    "      python_callable=my_function,\n",
    "      retries=3,\n",
    "      retry_delay=timedelta(minutes=5),\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "By implementing these methods, you can ensure that failures in your workflows are promptly detected and addressed, minimizing downtime and maintaining the reliability of your data pipelines. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be05ab-2696-4d20-91a9-c2e1a000a56b",
   "metadata": {},
   "source": [
    "### 7. What is your approach to reprocessing data in case of a failure?\n",
    "Reprocessing data in case of a failure is crucial for ensuring data integrity and consistency. Hereâ€™s a structured approach to handle reprocessing effectively:\n",
    "\n",
    "1. **Identify the Failure Point**\n",
    "- **Purpose**: Determine where the failure occurred in the data pipeline.\n",
    "- **Implementation**: Use logging and monitoring tools to pinpoint the exact stage or task that failed.\n",
    "- **Example**: Check logs in Databricks, Airflow, or any other orchestration tool to identify the failure.\n",
    "\n",
    "2. **Isolate the Affected Data**\n",
    "- **Purpose**: Identify the specific data that was affected by the failure.\n",
    "- **Implementation**: Use timestamps, versioning, or unique identifiers to isolate the data that needs reprocessing.\n",
    "- **Example**: Filter data based on a timestamp column to select only the records that were processed during the failure window.\n",
    "\n",
    "3. **Implement Idempotent Operations**\n",
    "- **Purpose**: Ensure that reprocessing the same data multiple times does not lead to inconsistencies.\n",
    "- **Implementation**: Design your data processing operations to be idempotent, meaning they can be applied multiple times without changing the result beyond the initial application.\n",
    "- **Example**: Use upserts (insert/update) instead of plain inserts to avoid duplicate records.\n",
    "\n",
    "4. **Use Checkpoints and Savepoints**\n",
    "- **Purpose**: Save intermediate states of your data pipeline to avoid reprocessing from the beginning.\n",
    "- **Implementation**: Use checkpointing and savepoint mechanisms provided by your data processing framework.\n",
    "- **Example**: In Spark, use `checkpoint()` to save the state of a DataFrame or RDD.\n",
    "  ```python\n",
    "  df.checkpoint()\n",
    "  ```\n",
    "\n",
    "5. **Automate Failure Recovery**\n",
    "- **Purpose**: Automate the reprocessing of data in case of a failure to minimize manual intervention.\n",
    "- **Implementation**: Use workflow orchestration tools to define retry policies and automate recovery steps.\n",
    "- **Example**: In Airflow, configure retries and define tasks to handle reprocessing.\n",
    "  ```python\n",
    "  task = PythonOperator(\n",
    "      task_id='my_task',\n",
    "      python_callable=my_function,\n",
    "      retries=3,\n",
    "      retry_delay=timedelta(minutes=5),\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "6. **Data Validation and Quality Checks**\n",
    "- **Purpose**: Ensure that the reprocessed data meets the required quality standards.\n",
    "- **Implementation**: Implement data validation and quality checks at various stages of the pipeline.\n",
    "- **Example**: Use assertions or custom validation functions to check data integrity.\n",
    "  ```python\n",
    "  assert df.count() > 0, \"DataFrame is empty\"\n",
    "  ```\n",
    "\n",
    "7. **Maintain Detailed Logs and Audit Trails**\n",
    "- **Purpose**: Keep detailed logs and audit trails of data processing activities for troubleshooting and compliance.\n",
    "- **Implementation**: Use logging frameworks and audit mechanisms to record processing details.\n",
    "- **Example**: Log the start and end times of each processing stage, along with any errors encountered.\n",
    "  ```python\n",
    "  import logging\n",
    "  logging.info(\"Starting data processing stage 1\")\n",
    "  ```\n",
    "\n",
    "8. **Communicate with Stakeholders**\n",
    "- **Purpose**: Keep stakeholders informed about the failure and the steps being taken to resolve it.\n",
    "- **Implementation**: Set up communication channels to notify stakeholders about the status of the reprocessing.\n",
    "- **Example**: Send email or Slack notifications to relevant teams.\n",
    "  ```python\n",
    "  from airflow.operators.email_operator import EmailOperator\n",
    "  email = EmailOperator(\n",
    "      task_id='send_email',\n",
    "      to='team@example.com',\n",
    "      subject='Data Pipeline Failure Alert',\n",
    "      html_content='The data pipeline has failed and reprocessing is in progress.',\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "By following these steps, you can effectively manage and reprocess data in case of a failure, ensuring data integrity and minimizing downtime. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3b7f1-c41f-4da6-86f4-64df2ede260e",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Can you explain the concept of the JVM and Python wrapper in Spark?\n",
    "**JVM (Java Virtual Machine)**: Spark runs on the JVM, which allows it to execute Java bytecode. The JVM provides a platform-independent way of running Java applications and is essential for Spark's performance and scalability.\n",
    "\n",
    "**Python Wrapper (PySpark)**: PySpark is the Python API for Spark. It allows Python developers to interact with Spark using Python code. PySpark uses a wrapper to communicate with the JVM, enabling Python code to execute Spark jobs. This involves serializing Python objects, sending them to the JVM, and deserializing the results back to Python.\n",
    "\n",
    "### 10. Why is it generally advised against using user-defined functions and data structures in Spark?\n",
    "User-defined functions (UDFs) and custom data structures can lead to performance issues in Spark because:\n",
    "- **Serialization Overhead**: UDFs require serialization and deserialization of data between the JVM and Python, which can be slow.\n",
    "- **Optimization Limitations**: Spark's Catalyst optimizer cannot optimize UDFs, leading to less efficient execution plans.\n",
    "- **Resource Consumption**: UDFs can consume more memory and CPU resources, impacting overall performance.\n",
    "\n",
    "### 11. What are the drawbacks of using user-defined functions in Spark?\n",
    "Drawbacks of using UDFs in Spark include:\n",
    "- **Performance Degradation**: UDFs can be slower due to serialization overhead and lack of optimization.\n",
    "- **Limited Optimization**: Spark's Catalyst optimizer cannot optimize UDFs, leading to less efficient execution plans.\n",
    "- **Complexity**: UDFs can make the code more complex and harder to maintain.\n",
    "- **Resource Consumption**: UDFs can consume more memory and CPU resources, impacting overall performance.\n",
    "\n",
    "### 12. Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?\n",
    "**Resilient Distributed Datasets (RDDs)** are the fundamental data structure in Spark. They are immutable, distributed collections of objects that can be processed in parallel. RDDs support two types of operations:\n",
    "- **Transformations**: Operations that create a new RDD from an existing one (e.g., `map`, `filter`).\n",
    "- **Actions**: Operations that trigger the execution of transformations and return a result (e.g., `collect`, `count`).\n",
    "\n",
    "### 13. What are actions and transformations in PySpark, and how do they differ?\n",
    "- **Transformations**: Operations that create a new DataFrame or RDD from an existing one. They are lazy, meaning they do not execute immediately. Examples include `map()`, `filter()`, and `groupByKey()`.\n",
    "- **Actions**: Operations that trigger the execution of the transformations and return a result to the driver program or write data to an external storage system. Examples include `collect()`, `count()`, and `saveAsTextFile()`.\n",
    "\n",
    "### 14. How do you manage and handle null values in PySpark DataFrames?\n",
    "To manage and handle null values in PySpark DataFrames, you can use the following methods:\n",
    "- **Drop Null Values**: Use `dropna()` to remove rows with null values.\n",
    "  ```python\n",
    "  df.dropna()\n",
    "  ```\n",
    "- **Fill Null Values**: Use `fillna()` to replace null values with a specified value.\n",
    "  ```python\n",
    "  df.fillna(0)\n",
    "  ```\n",
    "- **Filter Null Values**: Use `filter()` to filter out rows with null values.\n",
    "  ```python\n",
    "  df.filter(df[\"column\"].isNotNull())\n",
    "  ```\n",
    "\n",
    "### 15. What is a partition in PySpark, and how do you control partitioning for better performance?\n",
    "A **partition** in PySpark is a logical division of data that allows for parallel processing. Each partition is processed independently by a task. To control partitioning for better performance, you can:\n",
    "- **Repartition**: Use `repartition()` to increase or decrease the number of partitions.\n",
    "  ```python\n",
    "  df.repartition(10)\n",
    "  ```\n",
    "- **Coalesce**: Use `coalesce()` to reduce the number of partitions without causing a full shuffle.\n",
    "  ```python\n",
    "  df.coalesce(5)\n",
    "  ```\n",
    "\n",
    "### 16. Can you explain the difference between narrow and wide transformations in PySpark?\n",
    "- **Narrow Transformations**: Operations where each input partition contributes to only one output partition (e.g., `map()`, `filter()`). They do not require data shuffling.\n",
    "- **Wide Transformations**: Operations where each input partition contributes to multiple output partitions, causing a shuffle (e.g., `reduceByKey()`, `groupByKey()`).\n",
    "\n",
    "### 17. How does PySpark infer schemas, and what are the implications of this?\n",
    "PySpark can infer schemas using the `inferSchema` option. When enabled, Spark scans the data and determines the appropriate data types for each column. The implications of schema inference include:\n",
    "- **Convenience**: Automatically determines data types, reducing the need for manual schema definition.\n",
    "- **Performance**: Schema inference can be slow for large datasets, as it requires scanning the data.\n",
    "\n",
    "### 18. What role does SparkContext play in a PySpark application?\n",
    "**SparkContext** is the entry point for any Spark application. It allows you to create RDDs, accumulators, and broadcast variables, and it provides access to Spark's cluster. It is responsible for coordinating the execution of tasks on the cluster.\n",
    "\n",
    "### 19. How do you perform aggregations in PySpark, and what are the key considerations?\n",
    "To perform aggregations in PySpark, you can use the `groupBy()` and `agg()` methods:\n",
    "```python\n",
    "df.groupBy(\"column\").agg({\"column\": \"sum\"})\n",
    "```\n",
    "Key considerations for aggregations include:\n",
    "- **Partitioning**: Ensure data is partitioned appropriately to avoid skew and improve performance.\n",
    "- **Memory Usage**: Aggregations can consume significant memory, so monitor resource usage.\n",
    "\n",
    "### 20. What strategies do you use for caching data in PySpark to improve performance?\n",
    "Strategies for caching data in PySpark include:\n",
    "- **Cache**: Use `cache()` to store intermediate results in memory.\n",
    "  ```python\n",
    "  df.cache()\n",
    "  ```\n",
    "- **Persist**: Use `persist()` to store intermediate results with a specified storage level (e.g., memory, disk).\n",
    "  ```python\n",
    "  df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "  ```\n",
    "\n",
    "Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9a57d-1467-4ace-901b-b43db94727e7",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What cluster Manager you have used in your project?\n",
    "In my project, I have used **Apache YARN** as the cluster manager. YARN (Yet Another Resource Negotiator) is a resource management layer for Hadoop that allows multiple data processing engines to handle data stored in a single platform.\n",
    "\n",
    "### 2. What is your cluster Size?\n",
    "The cluster size can vary depending on the project's requirements. In my project, the cluster typically consists of **10-20 nodes**, with each node having multiple cores and a significant amount of memory to handle large-scale data processing tasks.\n",
    "\n",
    "### 3. How does your data come to your storage location?\n",
    "Data is ingested into the storage location using various methods, including:\n",
    "- **Batch Ingestion**: Using tools like Apache Sqoop to import data from relational databases.\n",
    "- **Streaming Ingestion**: Using Apache Kafka or Azure Event Hubs to stream data in real-time.\n",
    "- **File Transfer**: Using Azure Data Factory or AWS Glue to transfer files from on-premises systems to cloud storage.\n",
    "\n",
    "### 4. What are the other sources you have used in your project?\n",
    "Other sources used in the project include:\n",
    "- **Relational Databases**: SQL Server, MySQL, PostgreSQL.\n",
    "- **NoSQL Databases**: MongoDB, Cassandra.\n",
    "- **APIs**: RESTful APIs for external data sources.\n",
    "- **Cloud Storage**: Azure Blob Storage, AWS S3.\n",
    "\n",
    "### 5. What is the sink for your project?\n",
    "The sink for the project is typically a **data warehouse** or **data lake** where the processed data is stored for further analysis. Examples include:\n",
    "- **Azure Synapse Analytics**: For data warehousing.\n",
    "- **Azure Data Lake Storage**: For storing raw and processed data.\n",
    "\n",
    "### 6. What is the frequency of the data in your source?\n",
    "The frequency of data ingestion can vary:\n",
    "- **Batch Data**: Daily, hourly, or weekly.\n",
    "- **Streaming Data**: Real-time or near real-time (seconds to minutes).\n",
    "\n",
    "### 7. What is the volume of your data?\n",
    "The volume of data can vary depending on the project. In my project, the data volume typically ranges from **hundreds of gigabytes to terabytes** per day.\n",
    "\n",
    "### 8. Please explain your project in detail.\n",
    "In my project, we built a data pipeline to process and analyze customer transaction data. The pipeline involves the following steps:\n",
    "1. **Data Ingestion**: Data is ingested from various sources, including relational databases, APIs, and streaming platforms.\n",
    "2. **Data Storage**: Ingested data is stored in Azure Data Lake Storage.\n",
    "3. **Data Processing**: Using Apache Spark on Databricks, we process the data to clean, transform, and aggregate it.\n",
    "4. **Data Loading**: Processed data is loaded into Azure Synapse Analytics for further analysis and reporting.\n",
    "5. **Data Visualization**: Using Power BI to create dashboards and reports for business stakeholders.\n",
    "\n",
    "### 9. Let's say out of 100 tasks, 99 tasks completed however the last task is taking long hours to finish/complete, how to handle this issue?\n",
    "To handle this issue, you can:\n",
    "- **Enable Speculative Execution**: This allows Spark to launch speculative copies of slow tasks on other nodes.\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.speculation\", \"true\")\n",
    "  ```\n",
    "- **Check for Data Skew**: Ensure that data is evenly distributed across partitions to avoid skew.\n",
    "- **Increase Resources**: Allocate more memory and CPU resources to the task.\n",
    "\n",
    "### 10. What challenges have you faced and how did you overcome them?\n",
    "Challenges faced include:\n",
    "- **Data Quality Issues**: Implemented data validation and cleansing steps to ensure data quality.\n",
    "- **Performance Bottlenecks**: Optimized Spark jobs by tuning configurations and using efficient data structures.\n",
    "- **Scalability**: Used auto-scaling clusters to handle varying workloads.\n",
    "\n",
    "### 11. What optimization techniques have you used in your project and what is the reason for it?\n",
    "Optimization techniques used include:\n",
    "- **Caching and Persistence**: To avoid recomputation of intermediate results.\n",
    "- **Broadcast Joins**: To optimize joins with small tables.\n",
    "- **Partitioning**: To balance the workload across partitions.\n",
    "- **Predicate Pushdown**: To reduce the amount of data read from storage.\n",
    "\n",
    "### 12. Have you done Spark optimization tuning? If yes, how have you done that?\n",
    "Yes, Spark optimization tuning was done by:\n",
    "- **Adjusting Spark Configuration Parameters**: Tuning parameters like `spark.executor.memory`, `spark.executor.cores`, and `spark.sql.shuffle.partitions`.\n",
    "- **Using Efficient Data Formats**: Using Parquet and Delta formats for better performance.\n",
    "- **Optimizing Transformations**: Using narrow transformations and minimizing shuffles.\n",
    "\n",
    "### 13. Can you please walk me through the spark-submit command?\n",
    "The `spark-submit` command is used to submit Spark applications to a cluster. Here's an example:\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --name \"My Spark Application\" \\\n",
    "  --executor-memory 4g \\\n",
    "  --executor-cores 4 \\\n",
    "  --driver-memory 2g \\\n",
    "  --num-executors 10 \\\n",
    "  --conf spark.executor.memoryOverhead=512 \\\n",
    "  my_spark_app.py\n",
    "```\n",
    "This command specifies the master URL, deploy mode, application name, executor memory, executor cores, driver memory, number of executors, and additional configurations.\n",
    "\n",
    "### 14. Let's say you are getting your data volume is 100 GB, In your spark you are doing 5 Actions and 3 transformations on the data, explain what goes behind the scene with respect to Stages, tasks?\n",
    "When processing a 100 GB dataset in Spark with 5 actions and 3 transformations, Spark's execution model involves stages and tasks:\n",
    "- **Transformations**: Create a new DataFrame or RDD from an existing one. Examples include `map()`, `filter()`, and `groupByKey()`.\n",
    "- **Actions**: Trigger the execution of transformations. Examples include `count()`, `collect()`, and `saveAsTextFile()`.\n",
    "\n",
    "**Stages**:\n",
    "- **Definition**: A stage is a set of tasks that can be executed in parallel. Stages are determined by wide transformations that require shuffling data across the cluster.\n",
    "- **Formation**: Spark builds a Directed Acyclic Graph (DAG) of stages. Each stage contains tasks that can be executed without requiring data from other stages.\n",
    "\n",
    "**Tasks**:\n",
    "- **Definition**: A task is the smallest unit of work in Spark. Each stage is divided into tasks, where each task is executed on a partition of the data.\n",
    "- **Execution**: Tasks are executed by the executors on the worker nodes. The number of tasks in a stage is equal to the number of partitions in the DataFrame or RDD.\n",
    "\n",
    "### 15. How do you take your code to the higher environment?\n",
    "To take code to a higher environment, you can:\n",
    "- **Version Control**: Use Git for version control and branching.\n",
    "- **CI/CD Pipelines**: Use Azure DevOps or Jenkins to automate the build, test, and deployment process.\n",
    "- **Environment Configuration**: Ensure that environment-specific configurations are managed using environment variables or configuration files.\n",
    "\n",
    "### 16. How do you schedule your job in production?\n",
    "Jobs in production can be scheduled using:\n",
    "- **Apache Airflow**: For workflow orchestration and scheduling.\n",
    "- **Databricks Jobs**: For scheduling notebooks, JARs, or Python scripts.\n",
    "- **Azure Data Factory**: For scheduling data integration workflows.\n",
    "\n",
    "### 17. How do you reprocess the data if it failed?\n",
    "To reprocess data in case of failure:\n",
    "- **Identify the Failure Point**: Determine where the failure occurred.\n",
    "- **Isolate the Affected Data**: Identify the specific data that needs reprocessing.\n",
    "- **Retry Mechanisms**: Configure retry policies in the workflow orchestration tool.\n",
    "- **Automate Recovery**: Use automated scripts or pipelines to reprocess the data.\n",
    "\n",
    "### 18. Tell me one scenario you have gone wrong with your decision making and what you have learnt from that mistake?\n",
    "One scenario where I went wrong was underestimating the complexity of data transformations, leading to performance bottlenecks. I learned the importance of thorough testing and performance benchmarking before deploying to production.\n",
    "\n",
    "### 19. Let's say you have noticed duplicate records loaded in the table for the particular partition, how you resolved such issues?\n",
    "To resolve duplicate records:\n",
    "- **Identify Duplicates**: Use SQL queries to identify duplicate records.\n",
    "  ```sql\n",
    "  SELECT column, COUNT(*) FROM table GROUP BY column HAVING COUNT(*) > 1;\n",
    "  ```\n",
    "- **Remove Duplicates**: Use `DISTINCT` or `ROW_NUMBER()` to remove duplicates.\n",
    "  ```sql\n",
    "  DELETE FROM table WHERE rowid NOT IN (SELECT MIN(rowid) FROM table GROUP BY column);\n",
    "  ```\n",
    "### 20. What is the frequency of your jobs?\n",
    "The frequency of jobs can vary depending on the specific requirements of the project. Common frequencies include:\n",
    "- **Daily**: Jobs run once every day, typically during off-peak hours.\n",
    "- **Hourly**: Jobs run every hour to process near real-time data.\n",
    "- **Weekly**: Jobs run once a week, often for batch processing of large datasets.\n",
    "- **Custom Intervals**: Jobs can be scheduled to run at custom intervals, such as every 15 minutes or every 6 hours, based on the data processing needs.\n",
    "\n",
    "### 21. How do you notify your business/Stakeholders in case of any job failure?\n",
    "To notify business stakeholders in case of job failure, you can use various methods and tools:\n",
    "- **Email Notifications**: Set up email alerts to notify stakeholders when a job fails. This can be configured in workflow orchestration tools like Apache Airflow, Azure Data Factory, or Databricks.\n",
    "  ```python\n",
    "  from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "  email = EmailOperator(\n",
    "      task_id='send_email',\n",
    "      to='stakeholder@example.com',\n",
    "      subject='Job Failure Alert',\n",
    "      html_content='The job has failed. Please check the logs for details.',\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "- **Slack Notifications**: Use Slack APIs or integrations to send real-time alerts to a Slack channel.\n",
    "  ```python\n",
    "  from airflow.operators.slack_operator import SlackAPIPostOperator\n",
    "\n",
    "  slack_alert = SlackAPIPostOperator(\n",
    "      task_id='slack_alert',\n",
    "      token='your_slack_token',\n",
    "      channel='#alerts',\n",
    "      text='The job has failed. Please check the logs for details.',\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "- **PagerDuty or Opsgenie**: Use incident management tools to handle critical alerts and ensure timely responses.\n",
    "  ```python\n",
    "  from airflow.sensors.http_sensor import HttpSensor\n",
    "\n",
    "  pagerduty_alert = HttpSensor(\n",
    "      task_id='pagerduty_alert',\n",
    "      endpoint='https://events.pagerduty.com/v2/enqueue',\n",
    "      headers={\"Content-Type\": \"application/json\"},\n",
    "      http_conn_id='pagerduty',\n",
    "      request_params={\n",
    "          \"routing_key\": \"your_routing_key\",\n",
    "          \"event_action\": \"trigger\",\n",
    "          \"payload\": {\n",
    "              \"summary\": \"Job Failure Alert\",\n",
    "              \"severity\": \"critical\",\n",
    "              \"source\": \"Airflow\",\n",
    "              \"component\": \"job\"\n",
    "          }\n",
    "      },\n",
    "      dag=dag\n",
    "  )\n",
    "  ```\n",
    "\n",
    "- **Monitoring and Logging Tools**: Integrate with tools like Datadog, Prometheus, or Azure Monitor to track job execution and set up alerts for failures.\n",
    "\n",
    "By implementing these methods, you can ensure that stakeholders are promptly informed about job failures, allowing for quick resolution and minimal impact on business operations. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd1d27-0779-4640-871f-4fa4b2c7a490",
   "metadata": {},
   "source": [
    "### Lets say you are getting your data volume is 100 GB , In your spark you are doing 5 Actions and 3 transformations on the data, explain what goes behind the scene with respect to Stages ,tasks?\n",
    "When processing a 100 GB dataset in Spark with 5 actions and 3 transformations, Spark's execution model involves stages and tasks. Here's a detailed breakdown of what happens behind the scenes:\n",
    "\n",
    "Transformations and Actions\n",
    "- **Transformations**: These are operations that create a new DataFrame or RDD from an existing one. They are lazy, meaning they do not execute immediately. Examples include `map()`, `filter()`, and `groupByKey()`.\n",
    "- **Actions**: These are operations that trigger the execution of the transformations and return a result to the driver program or write data to an external storage system. Examples include `collect()`, `count()`, and `saveAsTextFile()`.\n",
    "\n",
    "Stages\n",
    "- **Definition**: A stage in Spark is a set of tasks that can be executed in parallel. Stages are determined by wide transformations (e.g., `reduceByKey()`, `join()`) that require shuffling data across the cluster.\n",
    "- **Formation**: When you perform transformations, Spark builds a Directed Acyclic Graph (DAG) of stages. Each stage contains tasks that can be executed without requiring data from other stages.\n",
    "\n",
    "Tasks\n",
    "- **Definition**: A task is the smallest unit of work in Spark. Each stage is divided into tasks, where each task is executed on a partition of the data.\n",
    "- **Execution**: Tasks are executed by the executors on the worker nodes. The number of tasks in a stage is equal to the number of partitions in the DataFrame or RDD.\n",
    "\n",
    "Example Scenario\n",
    "Let's break down the scenario with 5 actions and 3 transformations:\n",
    "\n",
    "1. **Initial Data Loading**:\n",
    "   - Spark reads the 100 GB dataset and partitions it. Let's assume it is partitioned into 1000 partitions.\n",
    "\n",
    "2. **Transformations**:\n",
    "   - **Transformation 1**: `map()` - This is a narrow transformation. Spark applies the `map()` function to each partition independently.\n",
    "   - **Transformation 2**: `filter()` - Another narrow transformation. Spark filters each partition independently.\n",
    "   - **Transformation 3**: `groupByKey()` - This is a wide transformation. Spark needs to shuffle data across the cluster to group records by key. This creates a new stage.\n",
    "\n",
    "3. **Actions**:\n",
    "   - **Action 1**: `count()` - Triggers the execution of the transformations. Spark executes the tasks in the stages created by the transformations.\n",
    "   - **Action 2**: `collect()` - Collects the data to the driver. This may involve additional stages if the data needs to be shuffled.\n",
    "   - **Action 3**: `saveAsTextFile()` - Writes the data to an external storage system. This may involve additional stages depending on the transformations applied.\n",
    "   - **Action 4**: `reduce()` - Another action that triggers execution. This may involve shuffling data if it requires aggregation.\n",
    "   - **Action 5**: `take()` - Retrieves a specified number of elements from the DataFrame or RDD. This may involve additional stages if the data needs to be shuffled.\n",
    "\n",
    "Execution Flow\n",
    "1. **Stage 1**: Executes the `map()` and `filter()` transformations. Each partition is processed independently, resulting in 1000 tasks.\n",
    "2. **Stage 2**: Executes the `groupByKey()` transformation. This involves shuffling data across the cluster, resulting in a new set of partitions and tasks.\n",
    "3. **Stage 3**: Executes the `count()` action. This involves aggregating the results from the previous stages.\n",
    "4. **Stage 4**: Executes the `collect()` action. This involves collecting the data to the driver.\n",
    "5. **Stage 5**: Executes the `saveAsTextFile()` action. This involves writing the data to an external storage system.\n",
    "6. **Stage 6**: Executes the `reduce()` action. This involves shuffling and aggregating the data.\n",
    "7. **Stage 7**: Executes the `take()` action. This involves retrieving a specified number of elements from the DataFrame or RDD.\n",
    "\n",
    "By understanding the stages and tasks involved in Spark's execution model, you can optimize your Spark applications for better performance and efficiency. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f134bef-1874-4350-850d-6662a2a7d079",
   "metadata": {},
   "source": [
    "### Lets say out of 100 task, 99 tasks completed however the last task is taking long hours to finish/complete, how to handle this issue ?1\n",
    "\n",
    "When one task out of many is taking significantly longer to complete, it can be a sign of a \"straggler\" task. Here are some strategies to handle this issue:\n",
    "\n",
    "1. **Speculative Execution**\n",
    "- **Purpose**: To mitigate the impact of slow tasks by launching speculative copies of the slow tasks on other nodes.\n",
    "- **Implementation**: Enable speculative execution in your Spark configuration.\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.speculation\", \"true\")\n",
    "  spark.conf.set(\"spark.speculation.quantile\", \"0.75\")\n",
    "  spark.conf.set(\"spark.speculation.multiplier\", \"1.5\")\n",
    "  ```\n",
    "\n",
    "2. **Data Skew Handling**\n",
    "- **Purpose**: To address data skew, where some partitions have significantly more data than others.\n",
    "- **Implementation**: Use techniques like salting to distribute data more evenly across partitions.\n",
    "  ```python\n",
    "  from pyspark.sql.functions import col, concat, lit\n",
    "  skewed_df = skewed_df.withColumn(\"salt\", (col(\"join_column\") % 10).cast(\"string\"))\n",
    "  skewed_df = skewed_df.withColumn(\"join_column_salted\", concat(col(\"join_column\"), col(\"salt\")))\n",
    "  ```\n",
    "\n",
    "3. **Resource Allocation**\n",
    "- **Purpose**: To ensure that the task has sufficient resources to complete efficiently.\n",
    "- **Implementation**: Increase the executor memory and cores if the task is resource-intensive.\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.executor.memory\", \"4g\")\n",
    "  spark.conf.set(\"spark.executor.cores\", \"4\")\n",
    "  ```\n",
    "\n",
    "4. **Task Repartitioning**\n",
    "- **Purpose**: To balance the workload more evenly across tasks.\n",
    "- **Implementation**: Repartition the DataFrame or RDD to create more evenly sized partitions.\n",
    "  ```python\n",
    "  df = df.repartition(100)\n",
    "  ```\n",
    "\n",
    "5. **Monitoring and Logging**\n",
    "- **Purpose**: To identify the root cause of the slow task.\n",
    "- **Implementation**: Use Spark's web UI and logs to monitor task execution and identify bottlenecks.\n",
    "\n",
    "6. **Optimizing Transformations**\n",
    "- **Purpose**: To reduce the complexity and execution time of transformations.\n",
    "- **Implementation**: Optimize the transformations to minimize shuffling and data movement.\n",
    "  ```python\n",
    "  df = df.filter(\"column > value\").select(\"column1\", \"column2\")\n",
    "  ```\n",
    "\n",
    "By implementing these strategies, you can handle slow tasks more effectively and ensure that your Spark jobs complete in a timely manner. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4b291-9a92-4444-ad78-96b3c32b5ec0",
   "metadata": {},
   "source": [
    "CREATE TABLE FIRST_TABLE(\n",
    "ID VARCHAR(10),\n",
    "DESCRIPTION VARCHAR(500),\n",
    "PRIMARY KEY( ID)\n",
    ");\n",
    "\n",
    "SELECT * FROM FIRST_TABLE;\n",
    "INSERT INTO FIRST_TABLE(ID,DESCRIPTION) VALUES(\"ABC\",\"hello 1 APPLE, oRAnge, Butter,milk, meat\");\n",
    "INSERT INTO FIRST_TABLE(ID, DESCRIPTION) VALUES(\"DEF\", \"hello 2 BANANA, grape, Cheese, yogurt, fish\");\n",
    "INSERT INTO FIRST_TABLE(ID, DESCRIPTION) VALUES(\"GHI\", \"hello 3 PEAR, lemon, Bread, cream, chicken\");\n",
    "INSERT INTO FIRST_TABLE(ID, DESCRIPTION) VALUES(\"MNO\", \"hello 5 STRAWBERRY, blueberry, Butter, milk, pork\");\n",
    "INSERT INTO FIRST_TABLE(ID, DESCRIPTION) VALUES(\"PQR\", \"hello 6 KIWI, watermelon, Butter, milk, lamb\");\n",
    "\n",
    "CREATE TABLE SECOND_TABLE(\n",
    "ID VARCHAR(10),\n",
    "CHECKPOINT VARCHAR(20),\n",
    "UNIQUEID INT,\n",
    "INFO VARCHAR(200),\n",
    "PRIMARY KEY(ID,CHECK)\n",
    ");\n",
    "\n",
    "INSERT INTO SECOND_TABLE(ID,CHECKPOINT,UNIQUEID,INFO) VALUES(\"ABC\",\"Included\",1, \"oRAnge\");\n",
    "INSERT INTO SECOND_TABLE(ID,CHECKPOINT,UNIQUEID,INFO) VALUES(\"ABC\",\"Included\",1, \"silk\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"DEF\", \"Included\", 2, \"apple\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"DEF\", \"Included\", 2, \"Bread\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"GHI\", \"Included\", 3, \"banana\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"MNO\", \"Included\", 5, \"pear\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"MNO\", \"Excluded\", 2, \"yogurt\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"PQR\", \"Included\", 6, \"mango\");\n",
    "INSERT INTO SECOND_TABLE(ID, CHECKPOINT, UNIQUEID, INFO) VALUES(\"VWX\", \"Included\", 8, \"pineapple\");\n",
    "\n",
    "WITH UNMATCHING AS (\n",
    "SELECT * FROM FIRST_TABLE A INNER JOIN SECOND_TABLE B ON A.ID = B.ID \n",
    "WHERE B.CHECKPOINT = 'Excluded'),\n",
    "matcing as(\n",
    "SELECT *\n",
    "FROM FIRST_TABLE C INNER JOIN SECOND_TABLE D ON C.ID = D.ID \n",
    "WHERE D.CHECKPOINT = 'Included')\n",
    "SELECT matcing.*  FROM MATCING A WHERE MATCING.DESCRIPTION NOT LIKE UNMATCHING.INFO ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3556d-23da-4acf-accc-a634ebf2cf28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
