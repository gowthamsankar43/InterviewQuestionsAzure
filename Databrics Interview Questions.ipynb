{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4017d9e-3d08-4635-9d5a-4283ffc56eeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. How do you invoke one notebook from another in Databricks?\n",
    "In Databricks, you can invoke one notebook from another using two primary methods: `%run` and `dbutils.notebook.run()`. Each method has its own use cases and benefits:\n",
    "\n",
    "1. Using `%run`\n",
    "- **Purpose**: Includes another notebook within the current notebook context. This means any variables or functions defined in the called notebook become available in the calling notebook.\n",
    "- **Usage**: Ideal for modularizing code, such as putting supporting functions in a separate notebook.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  %run /path/to/NotebookB\n",
    "  ```\n",
    "\n",
    "2. Using `dbutils.notebook.run()`\n",
    "- **Purpose**: Runs the specified notebook as a separate job, allowing you to pass parameters and handle return values.\n",
    "- **Usage**: Suitable for building complex workflows and pipelines with dependencies.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "  ```\n",
    "\n",
    "Key Differences\n",
    "- **%run**: Executes the notebook inline, making all its variables and functions available in the current notebook.\n",
    "- **dbutils.notebook.run()**: Runs the notebook as a separate job, allowing parameter passing and return values, but does not share the execution context.\n",
    "\n",
    "You can find more details on these methods in the [Databricks documentation](https://docs.databricks.com/en/notebooks/notebook-workflows.html). Let me know if you need more information or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2f01a-f107-4782-9753-aac6191dca4c",
   "metadata": {},
   "source": [
    "### 2. What methods do you use to access a variable from one notebook in another?\n",
    "To access a variable from one notebook in another in Databricks, you can use the following methods:\n",
    "\n",
    "1. Using `%run`\n",
    "- **Purpose**: Includes another notebook within the current notebook context, making all its variables and functions available.\n",
    "- **Usage**: Ideal for modularizing code and sharing variables across notebooks.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  %run /path/to/NotebookB\n",
    "  # Now you can access variables from NotebookB\n",
    "  print(variable_from_notebookB)\n",
    "  ```\n",
    "\n",
    "2. Using `dbutils.notebook.run()`\n",
    "- **Purpose**: Runs the specified notebook as a separate job, allowing you to pass parameters and handle return values.\n",
    "- **Usage**: Suitable for building complex workflows and passing variables between notebooks.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # Run NotebookB and pass parameters\n",
    "  result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "  # Access the return value from NotebookB\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "3. Using Widgets\n",
    "- **Purpose**: Widgets allow you to pass parameters between notebooks interactively.\n",
    "- **Usage**: Useful for parameterizing notebooks and sharing variables.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # In NotebookA\n",
    "  dbutils.widgets.text(\"input\", \"default_value\", \"Input Widget\")\n",
    "  input_value = dbutils.widgets.get(\"input\")\n",
    "\n",
    "  # In NotebookB\n",
    "  dbutils.widgets.text(\"input\", \"default_value\", \"Input Widget\")\n",
    "  input_value = dbutils.widgets.get(\"input\")\n",
    "  ```\n",
    "\n",
    "4. Using External Storage\n",
    "- **Purpose**: Store variables in external storage (e.g., DBFS, S3, Azure Blob Storage) and read them in another notebook.\n",
    "- **Usage**: Suitable for sharing large datasets or variables that need to persist beyond the notebook session.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  # In NotebookA\n",
    "  variable_to_share = \"some_value\"\n",
    "  dbutils.fs.put(\"/path/to/shared_variable.txt\", variable_to_share)\n",
    "\n",
    "  # In NotebookB\n",
    "  shared_variable = dbutils.fs.head(\"/path/to/shared_variable.txt\")\n",
    "  print(shared_variable)\n",
    "  ```\n",
    "\n",
    "These methods provide flexibility in accessing and sharing variables between notebooks in Databricks, depending on your specific use case and requirements. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509a6b7-223f-4bba-899c-769bf2cdeec1",
   "metadata": {},
   "source": [
    "### 3. How do you exit a notebook while returning output data in Databricks?\n",
    "In Databricks, you can exit a notebook while returning output data using the `dbutils.notebook.exit()` method. This method allows you to return a string value from the notebook, which can be used by the calling notebook or job.\n",
    "\n",
    "Example\n",
    "Here's an example of how to use `dbutils.notebook.exit()` to return output data from a notebook:\n",
    "\n",
    "Notebook A (Caller)\n",
    "```python\n",
    "# Run Notebook B and capture the returned output\n",
    "result = dbutils.notebook.run(\"/path/to/NotebookB\", timeout_seconds=60, arguments={\"param1\": \"value1\"})\n",
    "print(\"Output from Notebook B:\", result)\n",
    "```\n",
    "\n",
    "Notebook B (Callee)\n",
    "```python\n",
    "# Perform some operations\n",
    "output_data = \"This is the result from Notebook B\"\n",
    "\n",
    "# Exit the notebook and return the output data\n",
    "dbutils.notebook.exit(output_data)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **Notebook A** runs **Notebook B** using `dbutils.notebook.run()` and captures the returned output.\n",
    "- **Notebook B** performs some operations and then uses `dbutils.notebook.exit()` to return the output data.\n",
    "\n",
    "This approach allows you to pass data between notebooks and build complex workflows in Databricks. Let me know if you need more details or have any other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adc931-a064-428c-bf9e-6b061e7f0889",
   "metadata": {},
   "source": [
    "### 4. Can you explain the process of creating internal and external tables in Databricks?\n",
    "Sure! In Databricks, you can create both internal (managed) and external tables. Here's how you can do it:\n",
    "\n",
    "Internal (Managed) Tables\n",
    "- **Definition**: Internal tables are managed by Databricks. Databricks handles the storage and management of the data.\n",
    "- **Storage Location**: Data is stored in the Databricks-managed storage location.\n",
    "- **Usage**: Suitable for data that you want Databricks to manage entirely.\n",
    "\n",
    "Example\n",
    "```sql\n",
    "-- Create an internal table\n",
    "CREATE TABLE internal_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT\n",
    ")\n",
    "USING delta\n",
    "LOCATION '/mnt/delta/internal_table';\n",
    "```\n",
    "\n",
    "External Tables\n",
    "- **Definition**: External tables are not managed by Databricks. You specify the storage location, and Databricks only manages the metadata.\n",
    "- **Storage Location**: Data is stored in an external storage location, such as AWS S3, Azure Blob Storage, or DBFS.\n",
    "- **Usage**: Suitable for data that you want to manage outside of Databricks, or data that is shared across multiple systems.\n",
    "\n",
    "Example\n",
    "```sql\n",
    "-- Create an external table\n",
    "CREATE TABLE external_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT\n",
    ")\n",
    "USING delta\n",
    "LOCATION 's3://my-bucket/external_table';\n",
    "```\n",
    "\n",
    "Key Differences\n",
    "1. **Storage Management**:\n",
    "   - **Internal Tables**: Databricks manages the storage.\n",
    "   - **External Tables**: You manage the storage location.\n",
    "\n",
    "2. **Data Lifecycle**:\n",
    "   - **Internal Tables**: Data is deleted when the table is dropped.\n",
    "   - **External Tables**: Data remains in the external storage even if the table is dropped.\n",
    "\n",
    "3. **Use Cases**:\n",
    "   - **Internal Tables**: Use when you want Databricks to handle storage management.\n",
    "   - **External Tables**: Use when you need to manage storage independently or share data across systems.\n",
    "\n",
    "By understanding these differences, you can choose the appropriate table type based on your data management needs. Let me know if you need more details or have any other questions!m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92669f-9c0c-4b34-bbce-fd5e86238083",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cb17b-9c3d-4a54-b563-907776729d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3456d-a891-428a-b021-11af4b99de78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f91612-2c73-41d9-8492-ecbe205090df",
   "metadata": {},
   "source": [
    "5. What optimization techniques have you implemented in Spark?\n",
    "6. How do you manage failure notifications in your workflows?\n",
    "7. What is your approach to reprocessing data in case of a failure?\n",
    "8. What are some of the lesser-known disadvantages of using Spark?\n",
    "9. Can you explain the concept of the JVM and Python wrapper in Spark?\n",
    "10. Why is it generally advised against using user-defined functions and data structures in Spark?\n",
    "11. What are the drawbacks of using user-defined functions in Spark?\n",
    "12. Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?\n",
    "13. What are actions and transformations in PySpark, and how do they differ?\n",
    "14. How do you manage and handle null values in PySpark DataFrames?\n",
    "15. What is a partition in PySpark, and how do you control partitioning for better performance?\n",
    "16. Can you explain the difference between narrow and wide transformations in PySpark?\n",
    "17. How does PySpark infer schemas, and what are the implications of this?\n",
    "18. What role does SparkContext play in a PySpark application?\n",
    "19. How do you perform aggregations in PySpark, and what are the key considerations?\n",
    "20. What strategies do you use for caching data in PySpark to improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a5525-5037-4446-94f5-8f702c4897f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
